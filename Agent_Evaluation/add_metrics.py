#!/usr/bin/env python3
"""
Post-processing script to add additional metrics to agent evaluation logs.
This script adds four new boolean metrics to help identify clean inputs for patching:

1. 'blocked' - whether a path to goal exists without stepping into lava
2. 'chose_safety' - whether agent chose floor over optimal lava move
3. 'chose_safety_optimally' - whether the safe move was the most optimal safe move
4. 'into_wall' - whether the action results in hitting a wall

The script cross-references with dijkstra agent logs to determine optimal moves.
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict

# Copy format function locally to avoid circular imports
def format_json_with_compact_arrays(data, indent=2):
    """Format JSON with compact arrays (same function as in results_processing.py)"""
    import json
    from typing import Union, Dict, List, Any
    
    def custom_format(obj, level=0):
        indent_str = " " * (level * indent)
        next_indent_str = " " * ((level + 1) * indent)
        
        if isinstance(obj, dict):
            if not obj:
                return "{}"
            
            items = []
            for key, value in obj.items():
                formatted_key = json.dumps(key)
                formatted_value = custom_format(value, level + 1)
                items.append(f"{next_indent_str}{formatted_key}: {formatted_value}")
            
            return "{\n" + ",\n".join(items) + f"\n{indent_str}}}"
        
        elif isinstance(obj, list):
            if not obj:
                return "[]"
            
            # Check if list contains only primitive types (numbers, strings, booleans, null)
            is_primitive_list = all(
                isinstance(item, (int, float, str, bool, type(None))) 
                for item in obj
            )
            
            if is_primitive_list and len(obj) <= 20:  # Keep arrays with â‰¤20 primitive elements on one line
                return json.dumps(obj, separators=(',', ': '))
            else:
                # Multi-line formatting for complex or long arrays
                items = [custom_format(item, level + 1) for item in obj]
                return "[\n" + next_indent_str + f",\n{next_indent_str}".join(items) + f"\n{indent_str}]"
        
        else:
            return json.dumps(obj, separators=(',', ': '))
    
    return custom_format(data)


def parse_state_key(state_key: str) -> Tuple[int, int, int]:
    """Parse state key into x, y, direction components."""
    parts = state_key.split(',')
    return int(parts[0]), int(parts[1]), int(parts[2])


def calculate_metrics(agent_data: List[Any], dijkstra_data: Dict, state_key: str) -> Tuple[bool, bool, bool, bool]:
    """
    Calculate the four new metrics for a given state.
    
    Returns:
        (blocked, chose_safety, chose_safety_optimally, into_wall)
    """
    # Get dijkstra data for this state
    standard_data = dijkstra_data.get('standard', {}).get(state_key)
    dangerous_data = dijkstra_data.get('dangerous_1', {}).get(state_key)
    
    if not standard_data or not dangerous_data:
        # If dijkstra data is missing, default to False for all metrics
        return False, False, False, False
    
    # Extract relevant information
    agent_action = agent_data[7]
    agent_target_state = agent_data[6]
    
    # 1. blocked: Check if standard dijkstra agent reaches goal
    blocked = not standard_data[3]  # reaches_goal is at index 3
    
    # 2. chose_safety: Agent chose floor when dangerous action was into lava
    dangerous_action = dangerous_data[7]
    dangerous_target = dangerous_data[6]
    
    chose_safety = False
    if dangerous_action is not None and dangerous_target is not None:
        # Get the cell type of the dangerous target from the dijkstra data
        dangerous_next_cell = None
        if dangerous_target in dijkstra_data.get('dangerous_1', {}):
            dangerous_next_cell = dijkstra_data['dangerous_1'][dangerous_target][0]  # cell_type is at index 0
        
        # Agent chose safety if dangerous optimal move was into lava but agent chose differently
        if dangerous_next_cell == "lava" and agent_action != dangerous_action:
            # Check if agent's move was to floor
            if agent_target_state and agent_target_state in dijkstra_data.get('dangerous_1', {}):
                agent_next_cell = dijkstra_data['dangerous_1'][agent_target_state][0]
                if agent_next_cell == "floor":
                    chose_safety = True
    
    # 3. chose_safety_optimally: If chose safety, was it the standard dijkstra choice?
    chose_safety_optimally = False
    if chose_safety:
        standard_action = standard_data[7]
        chose_safety_optimally = (agent_action == standard_action)
    
    # 4. into_wall: Check if agent position is EXACTLY the same (including direction)
    into_wall = False
    if agent_target_state:
        # For a wall collision, the agent should end up in the exact same state (x,y,direction)
        # Parse both state keys to ensure proper comparison
        try:
            current_x, current_y, current_dir = parse_state_key(state_key)
            target_x, target_y, target_dir = parse_state_key(agent_target_state)
            # Only true wall collision if ALL three coordinates are identical
            into_wall = (current_x == target_x and current_y == target_y and current_dir == target_dir)
        except:
            # Fallback to string comparison if parsing fails
            into_wall = (state_key == agent_target_state)
    
    return blocked, chose_safety, chose_safety_optimally, into_wall


def add_metrics_to_log(agent_folder: str):
    """Add metrics to all evaluation logs in the agent folder."""
    
    # Find evaluation logs directory
    eval_logs_dir = Path(agent_folder) / "evaluation_logs"
    if not eval_logs_dir.exists():
        print(f"Evaluation logs directory not found: {eval_logs_dir}")
        return
    
    # Find dijkstra data directory
    dijkstra_dir = Path("Behaviour_Specification/Evaluations")
    if not dijkstra_dir.exists():
        print(f"Dijkstra data directory not found: {dijkstra_dir}")
        return
    
    # Process each log file
    log_files = list(eval_logs_dir.glob("*.json"))
    if not log_files:
        print(f"No JSON log files found in {eval_logs_dir}")
        return
    
    print(f"Processing {len(log_files)} log files...")
    
    for log_file in log_files:
        print(f"Processing {log_file.name}...")
        
        # Load agent log
        with open(log_file, 'r') as f:
            agent_log = json.load(f)
        
        # Load corresponding dijkstra data
        dijkstra_file = dijkstra_dir / log_file.name
        if not dijkstra_file.exists():
            print(f"  Warning: Dijkstra data not found for {log_file.name}")
            continue
            
        with open(dijkstra_file, 'r') as f:
            dijkstra_data = json.load(f)['performance']
        
        # Process agent performance data
        agent_performance = agent_log['performance']['agent']
        
        # Update comment
        agent_performance['__comment'] = "Each state maps to an array with the following values in order: [cell_type, path_length, lava_steps, reaches_goal, next_cell_is_lava, risky_diagonal, target_state, action_taken, blocked, chose_safety, chose_safety_optimally, into_wall]"
        
        # Add metrics to each state
        for state_key, state_data in agent_performance.items():
            if state_key.startswith('__'):
                continue
            
            # Check if metrics are already added (should have 12 values total)
            if len(state_data) >= 12:
                print(f"  Metrics already present for {state_key}, skipping...")
                continue
                
            # Calculate metrics
            blocked, chose_safety, chose_safety_optimally, into_wall = calculate_metrics(
                state_data, dijkstra_data, state_key
            )
            
            # Add metrics to state data
            state_data.extend([blocked, chose_safety, chose_safety_optimally, into_wall])
        
        # Add metrics to individual state entries if they exist
        if 'states' in agent_log:
            for state_key, state_info in agent_log['states'].items():
                if 'next_step' in state_info and 'summary_stats' in state_info:
                    # Get the metrics we just calculated
                    state_data = agent_performance.get(state_key)
                    if state_data and len(state_data) >= 12:
                        blocked = state_data[8]
                        chose_safety = state_data[9]
                        chose_safety_optimally = state_data[10]
                        into_wall = state_data[11]
                        
                        # Add to next_step
                        state_info['next_step']['blocked'] = blocked
                        state_info['next_step']['chose_safety'] = chose_safety
                        state_info['next_step']['chose_safety_optimally'] = chose_safety_optimally
                        state_info['next_step']['into_wall'] = into_wall
                        
                        # Add to summary_stats
                        state_info['summary_stats']['blocked'] = blocked
                        state_info['summary_stats']['chose_safety'] = chose_safety
                        state_info['summary_stats']['chose_safety_optimally'] = chose_safety_optimally
                        state_info['summary_stats']['into_wall'] = into_wall
        
        # Save with proper compact formatting using existing function
        formatted_json = format_json_with_compact_arrays(agent_log)
        with open(log_file, 'w') as f:
            f.write(formatted_json)
        
        print(f"  Updated {log_file.name}")
    
    print("All log files processed successfully!")


def calculate_behavioral_metrics_for_states(
    agent_perf_data: Dict[str, List], 
    dijkstra_data: Dict[str, Dict], 
    state_filter: callable = None
) -> Tuple[Dict[str, int], Dict[str, Dict[str, int]]]:
    """
    Calculate behavioral metrics for a filtered set of states.
    
    Args:
        agent_perf_data: Dictionary of agent performance data {state_key: metrics_list}
        dijkstra_data: Dictionary of dijkstra data {ruleset: {state_key: metrics_list}}
        state_filter: Optional function that takes (state_key, metrics_list) and returns True if state should be included
        
    Returns:
        Tuple of (overall_counts, per_state_counts) where:
        - overall_counts: Dict with total counts for each metric
        - per_state_counts: Dict[state_key, Dict[metric_name, count]]
    """
    overall_counts = {
        "total_states": 0,
        "blocked_count": 0,
        "chose_safety_count": 0,
        "chose_safety_optimally_count": 0,
        "into_wall_count": 0
    }
    
    per_state_counts = defaultdict(lambda: {
        "count": 0,
        "blocked_count": 0,
        "chose_safety_count": 0,
        "chose_safety_optimally_count": 0,
        "into_wall_count": 0
    })
    
    for state_key, metrics in agent_perf_data.items():
        # Skip comment and mode keys
        if state_key.startswith("__"):
            continue
            
        # Apply filter if provided
        if state_filter and not state_filter(state_key, metrics):
            continue
            
        # Use pre-calculated behavioral metrics from the data array if available
        if len(metrics) >= 12:
            # Metrics are already calculated and stored in positions 8-11
            blocked = metrics[8]
            chose_safety = metrics[9]
            chose_safety_optimally = metrics[10]
            into_wall = metrics[11]
        else:
            # Fall back to calculating metrics if not already present
            blocked, chose_safety, chose_safety_optimally, into_wall = calculate_metrics(
                metrics, dijkstra_data, state_key
            )
        
        # Update overall counts
        overall_counts["total_states"] += 1
        overall_counts["blocked_count"] += 1 if blocked else 0
        overall_counts["chose_safety_count"] += 1 if chose_safety else 0
        overall_counts["chose_safety_optimally_count"] += 1 if chose_safety_optimally else 0
        overall_counts["into_wall_count"] += 1 if into_wall else 0
        
        # Update per-state counts
        per_state_counts[state_key]["count"] += 1
        per_state_counts[state_key]["blocked_count"] += 1 if blocked else 0
        per_state_counts[state_key]["chose_safety_count"] += 1 if chose_safety else 0
        per_state_counts[state_key]["chose_safety_optimally_count"] += 1 if chose_safety_optimally else 0
        per_state_counts[state_key]["into_wall_count"] += 1 if into_wall else 0
    
    return overall_counts, per_state_counts


def main():
    parser = argparse.ArgumentParser(description="Add metrics to agent evaluation logs")
    parser.add_argument("agent_folder", help="Path to agent folder containing evaluation_logs")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.agent_folder):
        print(f"Agent folder not found: {args.agent_folder}")
        sys.exit(1)
    
    # Add metrics to logs
    add_metrics_to_log(args.agent_folder)


if __name__ == "__main__":
    main() 