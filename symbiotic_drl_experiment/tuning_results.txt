Saved model to tuned_models/ppo_lr3.2e-05_gamma0.958_clip0.225_nsteps128_ent2.5e-06_bs128_gae0.955_vf0.897.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=3.2e-05, gamma=0.958, clip_range=0.225, n_steps=128, ent_coef=2.5e-06, batch_size=128, gae_lambda=0.955, vf_coef=0.897
Saved model to tuned_models/ppo_lr1.0e-04_gamma0.923_clip0.253_nsteps512_ent1.5e-03_bs128_gae0.894_vf0.911.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=1.0e-04, gamma=0.923, clip_range=0.253, n_steps=512, ent_coef=1.5e-03, batch_size=128, gae_lambda=0.894, vf_coef=0.911
Saved model to tuned_models/ppo_lr2.3e-05_gamma0.904_clip0.113_nsteps1024_ent2.2e-06_bs256_gae0.981_vf0.459.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=2.3e-05, gamma=0.904, clip_range=0.113, n_steps=1024, ent_coef=2.2e-06, batch_size=256, gae_lambda=0.981, vf_coef=0.459
Saved model to tuned_models/ppo_lr1.2e-05_gamma0.955_clip0.153_nsteps512_ent4.3e-05_bs512_gae0.867_vf0.926.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=1.2e-05, gamma=0.955, clip_range=0.153, n_steps=512, ent_coef=4.3e-05, batch_size=512, gae_lambda=0.867, vf_coef=0.926
Saved model to tuned_models/ppo_lr6.3e-05_gamma0.933_clip0.256_nsteps256_ent4.5e-05_bs64_gae0.921_vf0.478.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=6.3e-05, gamma=0.933, clip_range=0.256, n_steps=256, ent_coef=4.5e-05, batch_size=64, gae_lambda=0.921, vf_coef=0.478
Saved model to tuned_models/ppo_lr7.5e-05_gamma0.987_clip0.155_nsteps128_ent9.2e-06_bs256_gae0.852_vf0.444.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=7.5e-05, gamma=0.987, clip_range=0.155, n_steps=128, ent_coef=9.2e-06, batch_size=256, gae_lambda=0.852, vf_coef=0.444
Saved model to tuned_models/ppo_lr3.1e-04_gamma0.982_clip0.100_nsteps1024_ent3.7e-04_bs128_gae0.886_vf0.940.zip with mean reward 0.97 and std reward 0.00 | hyperparameters: learning_rate=3.1e-04, gamma=0.982, clip_range=0.100, n_steps=1024, ent_coef=3.7e-04, batch_size=128, gae_lambda=0.886, vf_coef=0.940
Saved model to tuned_models/ppo_lr1.7e-05_gamma0.949_clip0.358_nsteps256_ent1.0e-03_bs256_gae0.963_vf0.210.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=1.7e-05, gamma=0.949, clip_range=0.358, n_steps=256, ent_coef=1.0e-03, batch_size=256, gae_lambda=0.963, vf_coef=0.210
Saved model to tuned_models/ppo_lr1.3e-05_gamma0.920_clip0.388_nsteps512_ent1.5e-03_bs256_gae0.956_vf0.639.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=1.3e-05, gamma=0.920, clip_range=0.388, n_steps=512, ent_coef=1.5e-03, batch_size=256, gae_lambda=0.956, vf_coef=0.639
Saved model to tuned_models/ppo_lr5.8e-05_gamma0.982_clip0.182_nsteps128_ent6.3e-05_bs256_gae0.901_vf0.466.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=5.8e-05, gamma=0.982, clip_range=0.182, n_steps=128, ent_coef=6.3e-05, batch_size=256, gae_lambda=0.901, vf_coef=0.466
Saved model to tuned_models/ppo_lr5.5e-04_gamma0.999_clip0.317_nsteps1024_ent1.9e-08_bs128_gae0.814_vf0.726.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=5.5e-04, gamma=0.999, clip_range=0.317, n_steps=1024, ent_coef=1.9e-08, batch_size=128, gae_lambda=0.814, vf_coef=0.726
Saved model to tuned_models/ppo_lr4.0e-04_gamma0.966_clip0.207_nsteps128_ent3.9e-07_bs128_gae0.926_vf0.997.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=4.0e-04, gamma=0.966, clip_range=0.207, n_steps=128, ent_coef=3.9e-07, batch_size=128, gae_lambda=0.926, vf_coef=0.997
Saved model to tuned_models/ppo_lr2.2e-04_gamma0.968_clip0.112_nsteps1024_ent3.0e-07_bs128_gae0.998_vf0.801.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=2.2e-04, gamma=0.968, clip_range=0.112, n_steps=1024, ent_coef=3.0e-07, batch_size=128, gae_lambda=0.998, vf_coef=0.801
Saved model to tuned_models/ppo_lr9.9e-04_gamma0.975_clip0.303_nsteps128_ent2.2e-04_bs128_gae0.849_vf0.810.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=9.9e-04, gamma=0.975, clip_range=0.303, n_steps=128, ent_coef=2.2e-04, batch_size=128, gae_lambda=0.849, vf_coef=0.810
Saved model to tuned_models/ppo_lr3.2e-05_gamma0.954_clip0.222_nsteps1024_ent7.4e-03_bs64_gae0.941_vf0.851.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=3.2e-05, gamma=0.954, clip_range=0.222, n_steps=1024, ent_coef=7.4e-03, batch_size=64, gae_lambda=0.941, vf_coef=0.851
Saved model to tuned_models/ppo_lr1.8e-04_gamma0.994_clip0.148_nsteps128_ent3.3e-06_bs512_gae0.885_vf0.663.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=1.8e-04, gamma=0.994, clip_range=0.148, n_steps=128, ent_coef=3.3e-06, batch_size=512, gae_lambda=0.885, vf_coef=0.663
Saved model to tuned_models/ppo_lr1.8e-04_gamma0.940_clip0.281_nsteps1024_ent2.8e-07_bs128_gae0.800_vf0.940.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=1.8e-04, gamma=0.940, clip_range=0.281, n_steps=1024, ent_coef=2.8e-07, batch_size=128, gae_lambda=0.800, vf_coef=0.940
Saved model to tuned_models/ppo_lr3.5e-05_gamma0.966_clip0.208_nsteps256_ent1.1e-08_bs128_gae0.922_vf0.737.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=3.5e-05, gamma=0.966, clip_range=0.208, n_steps=256, ent_coef=1.1e-08, batch_size=128, gae_lambda=0.922, vf_coef=0.737
Saved model to tuned_models/ppo_lr4.0e-04_gamma0.982_clip0.331_nsteps128_ent2.0e-04_bs128_gae0.837_vf0.991.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=4.0e-04, gamma=0.982, clip_range=0.331, n_steps=128, ent_coef=2.0e-04, batch_size=128, gae_lambda=0.837, vf_coef=0.991
Saved model to tuned_models/ppo_lr1.1e-04_gamma0.945_clip0.183_nsteps1024_ent9.7e-03_bs64_gae0.882_vf0.210.zip with mean reward 0.00 and std reward 0.00 | hyperparameters: learning_rate=1.1e-04, gamma=0.945, clip_range=0.183, n_steps=1024, ent_coef=9.7e-03, batch_size=64, gae_lambda=0.882, vf_coef=0.210
Hyperparameter optimization complete.
Best trial:
Best trial value: 0.97
learning_rate: 0.00030747611606124145
gamma: 0.9820351847009544
clip_range: 0.10042283443302924
n_steps: 1024
ent_coef: 0.00036929450881810987
batch_size: 128
gae_lambda: 0.8855664475177163
vf_coef: 0.9404787637957237
Model: ppo_lr3.1e-04_gamma0.982_clip0.100_nsteps1024_ent3.7e-04_bs128_gae0.886_vf0.940.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr5.8e-05_gamma0.982_clip0.182_nsteps128_ent6.3e-05_bs256_gae0.901_vf0.466.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr1.3e-05_gamma0.920_clip0.388_nsteps512_ent1.5e-03_bs256_gae0.956_vf0.639.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr3.2e-05_gamma0.954_clip0.222_nsteps1024_ent7.4e-03_bs64_gae0.941_vf0.851.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr1.2e-05_gamma0.955_clip0.153_nsteps512_ent4.3e-05_bs512_gae0.867_vf0.926.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr1.0e-04_gamma0.923_clip0.253_nsteps512_ent1.5e-03_bs128_gae0.894_vf0.911.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr1.8e-04_gamma0.994_clip0.148_nsteps128_ent3.3e-06_bs512_gae0.885_vf0.663.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr2.2e-04_gamma0.968_clip0.112_nsteps1024_ent3.0e-07_bs128_gae0.998_vf0.801.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr4.0e-04_gamma0.982_clip0.331_nsteps128_ent2.0e-04_bs128_gae0.837_vf0.991.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr1.7e-05_gamma0.949_clip0.358_nsteps256_ent1.0e-03_bs256_gae0.963_vf0.210.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr3.2e-05_gamma0.958_clip0.225_nsteps128_ent2.5e-06_bs128_gae0.955_vf0.897.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr1.1e-04_gamma0.945_clip0.183_nsteps1024_ent9.7e-03_bs64_gae0.882_vf0.210.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr5.5e-04_gamma0.999_clip0.317_nsteps1024_ent1.9e-08_bs128_gae0.814_vf0.726.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr1.8e-04_gamma0.940_clip0.281_nsteps1024_ent2.8e-07_bs128_gae0.800_vf0.940.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr9.9e-04_gamma0.975_clip0.303_nsteps128_ent2.2e-04_bs128_gae0.849_vf0.810.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr2.3e-05_gamma0.904_clip0.113_nsteps1024_ent2.2e-06_bs256_gae0.981_vf0.459.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr4.0e-04_gamma0.966_clip0.207_nsteps128_ent3.9e-07_bs128_gae0.926_vf0.997.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr7.5e-05_gamma0.987_clip0.155_nsteps128_ent9.2e-06_bs256_gae0.852_vf0.444.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr3.5e-05_gamma0.966_clip0.208_nsteps256_ent1.1e-08_bs128_gae0.922_vf0.737.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_lr6.3e-05_gamma0.933_clip0.256_nsteps256_ent4.5e-05_bs64_gae0.921_vf0.478.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
Model: ppo_doorkey.zip | Episode rewards: [np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)] | Mean reward: 0.00 | Std reward: 0.00
