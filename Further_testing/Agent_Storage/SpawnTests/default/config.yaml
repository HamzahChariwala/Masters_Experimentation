# Model Card for MiniGrid Agent Training
# Author: Hamzah Chariwala
# Created: 2023

# ===== General Configuration =====
experiment:
  name: "minigrid_agent_training"
  description: "Training an agent with flexible spawn distribution in MiniGrid"
  version: 1.0
  output:
    # The log_dir and model_save_path are now optional in our updated code
    # If --path is provided, logs will be stored in Agent_Storage/<path>/logs
    # and model will be saved to Agent_Storage/<path>/agent.zip
    
    # These can be left as fallbacks if --path is not provided
    log_dir: "./logs/default_logs"
    model_save_path: "default_agent"
    
    # Total timesteps remains important
    total_timesteps: 500000

# ===== Environment Configuration =====
environment:
  id: "MiniGrid-LavaCrossingS11N5-v0"  # Options: "MiniGrid-Empty-8x8-v0", "MiniGrid-LavaCrossingS9N1-v0"
  num_envs: 15
  max_episode_steps: 150  # Maximum steps per episode (None = use env default)
  use_different_envs: true  # Enable diverse environments

# ===== Reward Function Configuration =====
reward_function:
  enabled: true
  type: "linear"  # Options: "linear", "exponential", "sigmoid"
  x_intercept: 150  # Number of steps at which reward becomes approximately 0
  y_intercept: 1  # Initial reward value at step 0
  transition_width: 10  # Only used for sigmoid function
  verbose: false  # Controls whether basic information is printed to terminal
  debug_logging: false  # Controls whether detailed step-by-step logs are printed
  count_lava_steps: false  # Whether to count lava steps with a penalty
  lava_step_multiplier: 5.0  # How many regular steps one lava step is equivalent to

# ===== Randomness Control =====
seeds:
  model: 811       # For network initialization
  environment: 12345     # For environment generation
  evaluation: 67890     # For evaluation environments
  seed_increment: 1     # Increment between seeds for multiple environments (was 1000)

# ===== Agent Safety Features =====
no_death:
  enabled: true
  types: ["lava"]
  cost: -0.1   # Penalty for hitting death elements

# ===== Diagonal Movement =====
diagonal_moves:
  monitor: true
  success_reward: 0.0
  failure_penalty: 0

# ===== Spawn Distribution =====
spawn:
  use_flexible_spawn: false
  exclude_goal_adjacent: false
  
  # Fixed distribution (used when stage_training and continuous_transition are disabled)
  distribution_type: "gaussian_2d"  # Options: "uniform", "gaussian_2d", "corners", "border", "composite"
  distribution_params:
    center: [0.0, 0.0]  # Normalized coordinates [x, y] where (0,0) is top-left
    std: [0.05, 0.05]   # Smaller standard deviation for tighter concentration around top-left
  
  # Curriculum Learning Options
  stage_training:
    enabled: false
    num_stages: 8
    
    # Global curriculum controls
    curriculum_proportion: 0.8  # Fraction of total timesteps to use for curriculum (0.0-1.0)
                               # After this proportion, defaults to uniform random spawn
    
    # Transition controls
    smooth_transitions:
      enabled: false          # Disabled for performance improvement
      transition_proportion: 0.2  # Fraction of each stage's duration to use for transition
                                 # (e.g., 0.2 means 20% of stage time is used for transition)
      transition_rate: "linear"  # Options: "linear", "exponential", "sigmoid"
    
    distributions:
      # Stage 1: Gaussian centered around goal with high concentration
      - type: "gaussian_2d"
        params: 
          center: [1.0, 1.0]  # Goal position (bottom-right)
          std: [0.15, 0.15]   # Small standard deviation (tight concentration) - increased slightly
        description: "Goal-centered tight Gaussian"
        relative_duration: 1.0  # Relative duration of this stage
      
      # Stage 2: Same center but increase lobe size
      - type: "gaussian_2d"
        params:
          center: [1.0, 1.0]  # Still at goal
          std: [0.35, 0.35]   # Larger standard deviation - increased
        description: "Goal-centered wider Gaussian"
        relative_duration: 1.0  # Relative duration of this stage
      
      # Stage 3: Shift center to middle of env
      - type: "gaussian_2d"
        params:
          center: [0.5, 0.5]  # Center of the grid
          std: [0.25, 0.25]   # Same standard deviation as before
        description: "Center-positioned Gaussian"
        relative_duration: 1.0  # Relative duration of this stage
      
      # Stage 4: Same center but increase lobe size in direction connecting bottom left and top right corner
      - type: "gaussian_2d"
        params:
          center: [0.5, 0.5]        # Still at center
          std: [0.4, 0.2]           # Elongated std
          directional: true         # Use directional Gaussian
          angle: 135                # 135 degrees = bottom-left to top-right diagonal
        description: "Center-positioned directional Gaussian"
        relative_duration: 1.0  # Slightly longer relative duration
      
      # Stage 5: Stacked gaussians with equal weighting at bottom-left and top-right corners
      - type: "composite"
        params:
          distributions:
            - type: "gaussian_2d"
              weight: 0.5
              params:
                center: [0.0, 1.0]  # Bottom-left corner
                std: [0.2, 0.2]
            - type: "gaussian_2d"
              weight: 0.5
              params:
                center: [1.0, 0.0]  # Top-right corner
                std: [0.2, 0.2]
        description: "Bottom-left and top-right corner Gaussians"
        relative_duration: 1.0  # Longer relative duration for this complex stage
      
      # Stage 6: Revert to something similar to stage 3
      - type: "gaussian_2d"
        params:
          center: [0.5, 0.5]  # Back to center
          std: [0.3, 0.3]     # Slightly wider than stage 3
        description: "Center-positioned wider Gaussian"
        relative_duration: 1.0  # Relative duration of this stage
      
      # Stage 7: Gaussian centered around top left with medium lobe sizes
      - type: "gaussian_2d"
        params:
          center: [0.0, 0.0]  # Top-left corner
          std: [0.35, 0.35]   # Medium standard deviation - increased
        description: "Top-left corner medium Gaussian"
        relative_duration: 1.0  # Relative duration of this stage
      
      # Stage 8: Reduce lobe sizes to increase concentration around that corner
      - type: "gaussian_2d"
        params:
          center: [0.0, 0.0]  # Still at top-left
          std: [0.15, 0.15]   # Smaller standard deviation (more concentrated) - increased slightly
        description: "Top-left corner tight Gaussian"
        relative_duration: 1.0  # Slightly longer relative duration for final stage
  
  continuous_transition:
    enabled: false
    target_type: "uniform"
    rate: 1.0  # Transition rate (1.0 = linear)

# ===== Observation Configuration =====
observation:
  window_size: 7  # Size of the observation window
  # Choose which observation features to use
  cnn_keys: []
  mlp_keys: ["four_way_goal_direction", "four_way_angle_alignment", "barrier_mask", "lava_mask"]

# ===== Model Configuration =====
model:
  type: "DQN"
  policy: "MultiInputPolicy"
  
  # Hardware settings
  use_mps: false
  
  # Core parameters
  buffer_size: 100000
  learning_starts: 100_000
  batch_size: 64
  exploration_fraction: 0.5
  exploration_final_eps: 0.05
  gamma: 0.6
  learning_rate: 0.00025
  train_freq: 8
  target_update_interval: 1000
  verbose: 1
  
  # Feature extractor configuration
  features_extractor:
    class: "CustomCombinedExtractor"
    features_dim: 256
    cnn:
      num_layers: 1
      channels: [32]
      kernels: [3]
      strides: [1]
      paddings: [1]
    mlp:
      num_layers: 1
      hidden_sizes: [64]

# ===== Evaluation Settings =====
evaluation:
  # During training
  training:
    num_envs: 15
    check_freq: 50000       # Changed from 1_000_000 to 500_000 to get more evaluation points
    target_reward_threshold: 2.0     # Unreachable target to disable early termination
    min_reward_threshold: -1.0       # Set to a very low value to disable early termination
    max_runtime: 0                   # 0 = no time limit (disable time-based termination)
    n_eval_episodes: 1               # Keep running evaluations
    timeout: 15                      # seconds
    disable_early_stopping: true     # New flag to explicitly disable early stopping
    
  # Final evaluation
  final:
    num_envs: 10
    episodes_per_env: 3
    timeout: 3  # seconds 