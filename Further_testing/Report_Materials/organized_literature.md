# Comprehensive Literature References

## Table of Contents
1. [Reinforcement Learning Fundamentals](#reinforcement-learning-fundamentals)
2. [Mechanistic Interpretability](#mechanistic-interpretability)
3. [Neural Networks Architecture and Design](#neural-networks-architecture-and-design)
4. [Model Editing and Knowledge Patching](#model-editing-and-knowledge-patching)
5. [Representation Learning and Feature Engineering](#representation-learning-and-feature-engineering)
6. [Sparse Representations and Autoencoders](#sparse-representations-and-autoencoders)
7. [Explainability and Interpretability Methods](#explainability-and-interpretability-methods)
8. [Reward Shaping and Alignment](#reward-shaping-and-alignment)
9. [Policy Learning and Optimization](#policy-learning-and-optimization)
10. [Meta-Learning and Hyperparameter Optimization](#meta-learning-and-hyperparameter-optimization)
11. [Grid World and Environment Design](#grid-world-and-environment-design)
12. [Neuroscience Connections](#neuroscience-connections)
13. [Advanced Transformer Research](#advanced-transformer-research)
14. [Activation Engineering and Steering](#activation-engineering-and-steering)

---

## Reinforcement Learning Fundamentals

### Deep Reinforcement Learning Taxonomies
**Reference:** Li, B. (2020) 'Taxonomy of Reinforcement Learning Algorithms', GitHub Repository.
**URL:** https://github.com/bennylp/RL-Taxonomy

### Alternative RL Taxonomies
**Reference:** AI Learning Hub (2022) 'Taxonomy of Reinforcement Learning Algorithms', Medium.
**URL:** https://medium.com/@AILearningHub/taxonomy-of-reinforcement-learning-algorithms-9cc6ca39ea25

### Structured Categorization of RL Algorithms
**Reference:** Sharma, P. and Kumar, A. (2020) 'Taxonomy of Reinforcement Learning Algorithms', in Advanced Deep Learning Applications in Big Data Analytics, pp. 41-57.
**URL:** https://link.springer.com/chapter/10.1007/978-981-15-4095-0_3

### Visual Taxonomy of RL Algorithms
**Reference:** Unknown (2021) 'Taxonomy of Reinforcement Learning Algorithms', ResearchGate.
**URL:** https://www.researchgate.net/figure/Taxonomy-of-Reinforcement-Learning-Algorithms-DP-Dynamic-Programming-TD-Temporal_fig2_351096389

### Spinning Up in Deep RL
**Reference:** Achiam, J. (2018) 'Spinning Up in Deep RL', OpenAI.
**URL:** https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html

### Challenges in Deep Reinforcement Learning
**Reference:** Ibarz, B. et al. (2023) 'Unsolved Problems in Deep Reinforcement Learning', arXiv:2308.12438.
**URL:** https://arxiv.org/pdf/2308.12438

### Need for Interpretability
**Reference:** Unknown (2024) 'Need for Interpretability in AI Systems', ACM Digital Library.
**URL:** https://dl.acm.org/doi/abs/10.1145/3630106.3659037

### Partial Observability in RL
**Reference:** Wang, S. et al. (2023) 'Generalization in Partially Observable Environments via Efficient Learning of Dynamics', Proceedings of Machine Learning Research, 202, pp. 22736-22755.
**URL:** https://proceedings.mlr.press/v202/wang23p/wang23p.pdf

## Mechanistic Interpretability

### Introduction to Mechanistic Interpretability
**Reference:** Nanda, N. (2023) 'An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability Papers', Alignment Forum.
**URL:** https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite

### Mechanistic Interpretability Resources
**Reference:** Unknown (Unknown date) 'Mechanistic Interpretability Resources', Dynalist.
**URL:** https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J

### Getting Started with Mechanistic Interpretability
**Reference:** TransformerLens (2023) 'Getting Started with Mechanistic Interpretability', TransformerLens Documentation.
**URL:** https://transformerlensorg.github.io/TransformerLens/content/getting_started_mech_interp.html

### Indirect Object Identification
**Reference:** TransformerLens (Unknown date) 'Indirect Object Identification', Streamlit App.
**URL:** https://arena-ch1-transformers.streamlit.app/[1.3]_Indirect_Object_Identification

### Transformer Circuits
**Reference:** Anthropic (2022) 'Transformer Circuits', Anthropic Research.
**URL:** https://transformer-circuits.pub/

### Understanding RL Vision
**Reference:** Olah, C. et al. (2020) 'Understanding RL Vision', Distill.
**URL:** https://distill.pub/2020/understanding-rl-vision/

### Neural Networks Latent Structure
**Reference:** Geiger, A. et al. (2021) 'Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference', arXiv:2108.13138.
**URL:** https://arxiv.org/pdf/2108.13138

### Neural Network Interpretability
**Reference:** Lee, L. et al. (2023) 'Sparse Autoencoders Find Highly Interpretable Features in Language Models', arXiv:2302.12902.
**URL:** https://arxiv.org/abs/2302.12902

### Interpreting Network Features
**Reference:** Unknown (2024) 'Interpreting Neural Network Features', arXiv:2502.00684.
**URL:** https://arxiv.org/pdf/2502.00684

### Circuit Discovery Methods
**Reference:** Unknown (2023) 'Methods for Circuit Discovery in Neural Networks', arXiv:2301.04709.
**URL:** https://arxiv.org/pdf/2301.04709

### Advanced Feature Discovery
**Reference:** Unknown (2024) 'Advanced Feature Discovery in Neural Networks', arXiv:2406.14546.
**URL:** https://arxiv.org/pdf/2406.14546

### Scaling and Emergent Features
**Reference:** Unknown (2023) 'Scaling and Emergent Features in Neural Networks', arXiv:2305.19911.
**URL:** https://arxiv.org/pdf/2305.19911

### Curve Detectors in CNNs
**Reference:** Cammarata, N. et al. (2020) 'Curve Detectors', Distill.
**URL:** https://distill.pub/2020/circuits/curve-detectors/

### Network Feature Analysis
**Reference:** Unknown (2024) 'Network Feature Analysis Methods', arXiv:2402.04362.
**URL:** https://arxiv.org/pdf/2402.04362

### Identifying Key Features
**Reference:** Unknown (2023) 'Identifying Key Features in Neural Networks', arXiv:2312.06581.
**URL:** https://arxiv.org/pdf/2312.06581

### Latent Space Structure
**Reference:** Unknown (2023) 'Latent Space Structure in Neural Networks', arXiv:2302.03025.
**URL:** https://arxiv.org/pdf/2302.03025

### Monosemantic Features
**Reference:** Anthropic (2023) 'Monosemantic Features', Transformer Circuits.
**URL:** https://transformer-circuits.pub/2023/monosemantic-features

### Feature Activation Patterns
**Reference:** Unknown (2024) 'Feature Activation Patterns in Neural Networks', arXiv:2406.00877.
**URL:** https://arxiv.org/pdf/2406.00877

### Neural Network Feature Analysis
**Reference:** Unknown (Unknown date) 'Neural Network Feature Analysis', OpenReview.
**URL:** https://openreview.net/pdf?id=fSbPwHjdDG

### Scaling Monosemanticity
**Reference:** Anthropic (2024) 'Scaling Monosemanticity', Transformer Circuits.
**URL:** https://transformer-circuits.pub/2024/scaling-monosemanticity/

### Finding Key Neurons
**Reference:** Unknown (2020) 'Finding Key Neurons in Neural Networks', arXiv:2002.09815.
**URL:** https://arxiv.org/pdf/2002.09815

### Causal Scrubbing
**Reference:** Gao, S. and Chan, L. (2022) 'Causal Scrubbing: A Method for Rigorously Testing Interpretability Hypotheses', LessWrong.
**URL:** https://www.lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing

### Attribution Patching
**Reference:** Nanda, N. (Unknown date) 'Attribution Patching: Does This Work in Practice? Experiments', Neel Nanda's website.
**URL:** https://www.neelnanda.io/mechanistic-interpretability/attribution-patching#does-this-work-in-practice-experiments

## Neural Networks Architecture and Design

### Deep RL Architectures
**Reference:** Sherstov, A.A. and Stone, P. (2017) 'Function Approximation via Sparse Distributed Memory', arXiv:1705.10528.
**URL:** https://arxiv.org/pdf/1705.10528

### Alternative DRL Architectures
**Reference:** Unknown (2023) 'Alternative Deep Reinforcement Learning Architectures', arXiv:2301.03044.
**URL:** https://arxiv.org/pdf/2301.03044

### Hypernetworks and Weight Space
**Reference:** Von Oswald, J. et al. (2021) 'Learning to Learn with Feedback and Local Plasticity', arXiv:2107.05479.
**URL:** https://arxiv.org/pdf/2107.05479

### Advanced Hypernetworks
**Reference:** Unknown (2023) 'Advanced Hypernetworks for Neural Networks', arXiv:2306.06955.
**URL:** https://arxiv.org/pdf/2306.06955

### Hypernetwork Applications
**Reference:** Unknown (2024) 'Hypernetwork Applications in AI Systems', arXiv:2406.09413.
**URL:** https://arxiv.org/pdf/2406.09413

### Hypernetwork Learning Methods
**Reference:** Unknown (2022) 'Hypernetwork Learning Methods', arXiv:2211.15457.
**URL:** https://arxiv.org/abs/2211.15457

### Autoencoder Embedding Methods
**Reference:** Jing, L. and Tian, Y. (2021) 'Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey', IEEE TPAMI, 43(11), pp. 4037-4058.
**URL:** https://arxiv.org/pdf/2106.14467

### Autoencoder Applications
**Reference:** Unknown (2019) 'Autoencoder Applications in Deep Learning', arXiv:1908.09257.
**URL:** https://arxiv.org/pdf/1908.09257

### Sparsification Methods
**Reference:** Unknown (2018) 'Sparsification Methods for Neural Networks', arXiv:1803.03635.
**URL:** https://arxiv.org/pdf/1803.03635

### Adapters for Transfer Learning
**Reference:** Ruder, S. (Unknown date) 'Adapters: A Compact and Extensible Transfer Learning Method for NLP', DAIR.AI.
**URL:** https://medium.com/dair-ai/adapters-a-compact-and-extensible-transfer-learning-method-for-nlp-6d18c2399f62

### Transcoders
**Reference:** Unknown (2024) 'Transcoders for Neural Network Adaptation', arXiv:2406.11944.
**URL:** https://arxiv.org/pdf/2406.11944

## Model Editing and Knowledge Patching

### Network Patching Techniques
**Reference:** Sotos, A. et al. (2023) 'Verifiable Neural Network Repair', arXiv:2505.02629.
**URL:** https://arxiv.org/pdf/2505.02629

### AIRepair
**Reference:** Ling, Z. et al. (2023) 'AIRepair: Fixing Neural Network Models without Retraining', Proceedings of the 45th ICSE.
**URL:** https://ssvlab.github.io/lucasccordeiro/papers/icse2023.pdf

### Neural Network Repair Methods
**Reference:** Unknown (2019) 'Neural Network Repair Methods', UC Davis Publications.
**URL:** https://thakur.cs.ucdavis.edu/assets/pubs/SRDM2019.pdf

### MEND: Model Editing with Minimal Data
**Reference:** Mitchell, E. et al. (2022) 'Fast Model Editing at Scale', ICLR.
**URL:** https://openreview.net/pdf?id=0DcZxeWfOPt

### Advanced Model Editing
**Reference:** Unknown (Unknown date) 'Advanced Model Editing Methods', OpenReview.
**URL:** https://openreview.net/pdf?id=1dkL3MVBfV

### NeuRecover
**Reference:** Unknown (2022) 'NeuRecover: Neural Network Recovery Methods', arXiv:2203.00191.
**URL:** https://arxiv.org/pdf/2203.00191

### VeRe: Verified Repair
**Reference:** Unknown (2023) 'VeRe: Verified Repair of Neural Networks', ACM Digital Library.
**URL:** https://dl.acm.org/doi/pdf/10.1145/3597503.3623332

### Arachne
**Reference:** Unknown (2019) 'Arachne: Neural Network Repair Framework', arXiv:1912.12463.
**URL:** https://arxiv.org/pdf/1912.12463

### Neural Network Bug Detection
**Reference:** Unknown (Unknown date) 'Neural Network Bug Detection and Repair', Iowa State University.
**URL:** https://dr.lib.iastate.edu/server/api/core/bitstreams/77548205-77dc-4c13-9538-25d59552a42c/content

### Knowledge Editing
**Reference:** Wang, T. et al. (2023) 'Evaluating and Inducing Personality in Pre-trained Language Models', arXiv:2310.19704v3.
**URL:** https://arxiv.org/pdf/2310.19704v3

### Advanced Knowledge Editing
**Reference:** Unknown (2024) 'Advanced Knowledge Editing in Neural Networks', arXiv:2401.01286.
**URL:** https://arxiv.org/pdf/2401.01286

### Weight Updates
**Reference:** Unknown (2024) 'Weight Update Methods for Neural Networks', arXiv:2410.17146.
**URL:** https://arxiv.org/pdf/2410.17146

### LLM Post-Training Methods
**Reference:** Unknown (2024) 'LLM Post-Training Methods', arXiv:2503.06072.
**URL:** https://arxiv.org/pdf/2503.06072

### Adapt-LLM
**Reference:** Unknown (Unknown date) 'Adapt-LLM: Efficient Adaptation of Large Language Models', Adapt-LLM Project.
**URL:** https://adapt-llm.github.io

### LLM Post-Training Introduction
**Reference:** Unknown (2024) 'Introduction to LLM Post-Training Methods', arXiv:2504.15133.
**URL:** https://arxiv.org/pdf/2504.15133

### Fine-Tuning Techniques
**Reference:** Unknown (2024) 'Fine-Tuning Techniques for Neural Networks', arXiv:2504.02620.
**URL:** https://arxiv.org/pdf/2504.02620

### Casual Layer Attribution
**Reference:** Unknown (2024) 'Casual Layer Attribution in Neural Networks', arXiv:2504.02976.
**URL:** https://arxiv.org/pdf/2504.02976

### Elastic Weight Consolidation
**Reference:** Kirkpatrick, J. et al. (2021) 'Understanding the Mechanisms of Catastrophic Forgetting in Neural Networks', arXiv:2105.04093.
**URL:** https://arxiv.org/pdf/2105.04093

## Representation Learning and Feature Engineering

### Linear Representation Hypothesis
**Reference:** Bellemare, M.G. et al. (2021) 'On Feature Hostility and Pipeline Conditioning in Online Reinforcement Learning', arXiv:2104.07143.
**URL:** https://arxiv.org/pdf/2104.07143

### Linear Representations Analysis
**Reference:** Unknown (2024) 'Linear Representations in Deep Learning', arXiv:2408.10920.
**URL:** https://arxiv.org/pdf/2408.10920

### Feature Learning Assessment
**Reference:** Unknown (2023) 'Assessment of Feature Learning in Neural Networks', arXiv:2309.00941.
**URL:** https://arxiv.org/pdf/2309.00941

### Linear Representation Advancements
**Reference:** Unknown (2023) 'Advancements in Linear Representation for Neural Networks', arXiv:2311.03658.
**URL:** https://arxiv.org/pdf/2311.03658

### Latent Space Search
**Reference:** Unknown (2023) 'Latent Space Search Methods', arXiv:2311.13569.
**URL:** https://arxiv.org/abs/2311.13569

### Entropy Maximization
**Reference:** Haarnoja, T. et al. (2018) 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor', ICML, pp. 1861-1870.
**URL:** https://proceedings.mlr.press/v80/haarnoja18a/haarnoja18a.pdf

### Adversarial Representations
**Reference:** Unknown (2022) 'Adversarial Representation Learning', arXiv:2207.09855.
**URL:** https://arxiv.org/abs/2207.09855

### Latent Space Optimization
**Reference:** Unknown (2024) 'Latent Space Optimization Methods', arXiv:2407.15549.
**URL:** https://arxiv.org/pdf/2407.15549

### Task Vectors
**Reference:** Unknown (2023) 'Task Vectors for Neural Networks', arXiv:2310.15213.
**URL:** https://arxiv.org/abs/2310.15213

### Advanced Task Vectors
**Reference:** Unknown (2023) 'Advanced Methods for Task Vectorization', arXiv:2310.15916.
**URL:** https://arxiv.org/pdf/2310.15916

### Function Vectors
**Reference:** Unknown (2024) 'Function Vectors in Neural Networks', arXiv:2406.11717.
**URL:** https://arxiv.org/pdf/2406.11717

### Representation Engineering
**Reference:** Zou, A. et al. (2023) 'Representation Engineering: A Top-Down Approach to AI Transparency', arXiv:2310.01405.
**URL:** https://arxiv.org/pdf/2310.01405

### Optimal Transport
**Reference:** Unknown (2023) 'Optimal Transport for Representation Learning', arXiv:2306.16156.
**URL:** https://arxiv.org/pdf/2306.16156

## Sparse Representations and Autoencoders

### Sparse Autoencoders
**Reference:** Lee, M. et al. (2024) 'Sparse Autoencoders Learn Features That Help Mechanistic Interpretability', arXiv:2405.08366.
**URL:** https://arxiv.org/pdf/2405.08366

### Meta-SAEs
**Reference:** Bricken, T. (2023) 'Showing SAE Latents Are Not Atomic Using Meta-SAEs', LessWrong.
**URL:** https://www.lesswrong.com/posts/TMAmHh4DdMr4nCSr5/showing-sae-latents-are-not-atomic-using-meta-saes

### Sparse Autoencoder Evaluation
**Reference:** Unknown (Unknown date) 'Evaluation of Sparse Autoencoders', OpenReview.
**URL:** https://openreview.net/pdf?id=VJ66JyKxgp

### Sparse Autoencoder Applications
**Reference:** Unknown (2024) 'Applications of Sparse Autoencoders', arXiv:2412.06410.
**URL:** https://arxiv.org/pdf/2412.06410

### Efficient Sparse Autoencoders
**Reference:** Unknown (2024) 'Efficient Sparse Autoencoder Implementations', arXiv:2406.04093.
**URL:** https://arxiv.org/pdf/2406.04093

### Advanced Sparse Autoencoders
**Reference:** Unknown (2024) 'Advanced Sparse Autoencoder Methods', arXiv:2407.14435.
**URL:** https://arxiv.org/pdf/2407.14435

### Sparse Autoencoder Analysis
**Reference:** Unknown (2024) 'Analysis of Sparse Autoencoders', arXiv:2404.16014.
**URL:** https://arxiv.org/pdf/2404.16014

### Sparse Feature Circuits
**Reference:** Unknown (2024) 'Sparse Feature Circuits in Neural Networks', arXiv:2403.19647.
**URL:** https://arxiv.org/pdf/2403.19647

## Explainability and Interpretability Methods

### Self-interpretable Neural Networks
**Reference:** Unknown (2024) 'Self-interpretable Neural Networks', arXiv:2501.15638v2.
**URL:** https://arxiv.org/pdf/2501.15638v2

### Interpretable Model Design
**Reference:** Unknown (Unknown date) 'Interpretable Model Design', OpenReview.
**URL:** https://openreview.net/pdf?id=NboGncLRV1

### Self-Explaining Neural Networks
**Reference:** Alvarez-Melis, D. and Jaakkola, T. (2018) 'Towards Robust Interpretability with Self-Explaining Neural Networks', arXiv:1806.07538.
**URL:** https://arxiv.org/pdf/1806.07538

### DRL Explainability
**Reference:** Puiutta, E. and Veith, E. (2020) 'Explainable Reinforcement Learning: A Survey', International Cross-Domain Conference for Machine Learning and Knowledge Extraction, pp. 77-95.
**URL:** https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9107404

### Recent DRL Explainability
**Reference:** Unknown (2024) 'Recent Advances in Explainable Reinforcement Learning', arXiv:2411.12173.
**URL:** https://arxiv.org/pdf/2411.12173

### Explainable RL Methods
**Reference:** Unknown (2024) 'Explainable RL Methods', arXiv:2411.16120.
**URL:** https://arxiv.org/pdf/2411.16120

### Interpretable RL Agents
**Reference:** Unknown (2025) 'Interpretable RL Agents', arXiv:2501.03142.
**URL:** https://arxiv.org/pdf/2501.03142

### Explainability Frameworks
**Reference:** Unknown (2025) 'Explainability Frameworks for RL', arXiv:2501.09858.
**URL:** https://arxiv.org/pdf/2501.09858

### Explainable RL Advances
**Reference:** Unknown (2025) 'Advances in Explainable RL', arXiv:2502.06869v1.
**URL:** https://arxiv.org/pdf/2502.06869v1

### Surveys on XRL
**Reference:** Unknown (Unknown date) 'Surveys on Explainable Reinforcement Learning', ACM Digital Library.
**URL:** https://dl.acm.org/doi/pdf/10.1145/3616864

### Feature Attribution for RL
**Reference:** Unknown (2020) 'Feature Attribution for Reinforcement Learning', NeurIPS.
**URL:** https://proceedings.neurips.cc/paper_files/paper/2020/file/1bd69c7df3112fb9a584fbd9edfc6c90-Paper.pdf

### LLM Explainability
**Reference:** Qian, Y. et al. (2024) 'A Survey on Explainable Large Language Models: Taxonomy, Methods, and Applications', arXiv:2502.14133.
**URL:** https://arxiv.org/pdf/2502.14133

### Advanced LLM Explainability
**Reference:** Unknown (2025) 'Advanced LLM Explainability Methods', arXiv:2504.02685.
**URL:** https://arxiv.org/pdf/2504.02685

### Interpretable Policies
**Reference:** Unknown (Unknown date) 'Iterative Bounding MDPs: Learning Interpretable Policies via Non-Interpretable Methods', ResearchGate.
**URL:** https://www.researchgate.net/publication/363402542_Iterative_Bounding_MDPs_Learning_Interpretable_Policies_via_Non-Interpretable_Methods

### Interpretable Policies in POMDP
**Reference:** Fox, L. et al. (2024) 'Learning Interpretable Policies in Partially Observable Environments', arXiv:2501.03888.
**URL:** https://arxiv.org/pdf/2501.03888

### Interpretable Policy Learning
**Reference:** Unknown (2024) 'Interpretable Policy Learning Methods', arXiv:2405.14956.
**URL:** https://arxiv.org/pdf/2405.14956

### Saliency Maps
**Reference:** Puri, N. et al. (2020) 'Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution', arXiv:1912.05743.
**URL:** https://arxiv.org/pdf/1912.05743

## Reward Shaping and Alignment

### Reward Reports
**Reference:** Turner, A. et al. (2024) 'Reward Reports for Reinforcement Learning', arXiv:2412.16325.
**URL:** https://www.arxiv.org/pdf/2412.16325

### Reward Tampering
**Reference:** Everitt, T. et al. (2019) 'Reward Tampering Problems and Solutions in Reinforcement Learning', arXiv:1908.04734.
**URL:** https://arxiv.org/pdf/1908.04734

### Agent Incentives Design
**Reference:** DeepMind Safety Research (Unknown date) 'Designing Agent Incentives to Avoid Reward Tampering', Medium.
**URL:** https://deepmindsafetyresearch.medium.com/designing-agent-incentives-to-avoid-reward-tampering-4380c1bb6cd

### Alignment Challenges
**Reference:** Unknown (2024) 'Challenges in RL Alignment', arXiv:2403.03185v4.
**URL:** https://arxiv.org/pdf/2403.03185v4

### Effective Reward Shaping
**Reference:** Unknown (2024) 'Effective Reward Shaping Methods', arXiv:2408.10215v1.
**URL:** https://arxiv.org/pdf/2408.10215v1

### Stable Reward Shaping
**Reference:** Zou, H. et al. (2023) 'Effective and Stable Reward Shaping in Reinforcement Learning', arXiv:2312.01072v1.
**URL:** https://arxiv.org/pdf/2312.01072v1

### Classic Reward Shaping
**Reference:** Unknown (Unknown date) 'Classic Approaches to Reward Shaping', CiteSeerX.
**URL:** https://citeseerx.ist.psu.edu/document?doi=a5ca908bac6dcc907a700d4e975dfc7e51a3fd89&repid=rep1&type=pdf

### Reward Shaping Analysis
**Reference:** Unknown (2024) 'Analysis of Reward Shaping Techniques', arXiv:2410.12197.
**URL:** https://arxiv.org/pdf/2410.12197

### State-Based Reward Shaping
**Reference:** Unknown (2021) 'State-Based Reward Shaping', arXiv:2106.10268.
**URL:** https://arxiv.org/pdf/2106.10268

### Policy-Based Reward Shaping
**Reference:** Unknown (2020) 'Policy-Based Reward Shaping', arXiv:2002.12292.
**URL:** https://arxiv.org/pdf/2002.12292

### Potential-Based Reward Shaping
**Reference:** Ng, A.Y., Harada, D. and Russell, S. (1999) 'Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping', ICML, pp. 278-287.
**URL:** https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf

### Function Approximation Reward Shaping
**Reference:** Unknown (2011) 'Function Approximation for Reward Shaping', arXiv:1106.5267.
**URL:** https://arxiv.org/pdf/1106.5267

### RHLF Issues
**Reference:** Unknown (2023) 'Issues in Reinforcement Learning from Human Feedback', arXiv:2307.15217.
**URL:** https://arxiv.org/pdf/2307.15217

### Human Feedback Systems
**Reference:** Fox, L. (2024) 'Learning from Human Feedback in Reinforcement Learning', Warwick University.
**URL:** https://wrap.warwick.ac.uk/id/eprint/190359/1/WRAP_Theses_Fox_2024.pdf

### RHLF Challenges
**Reference:** Unknown (2024) 'Challenges in Reinforcement Learning from Human Feedback', arXiv:2412.10400.
**URL:** https://arxiv.org/pdf/2412.10400

### Safe Exploration
**Reference:** Singh, S. et al. (2022) 'Deep Reinforcement Learning with Self-Supervising Barriers for Safe Exploration', ICLR.
**URL:** https://openreview.net/pdf?id=dQLsvKNwZC

### Inverse RL
**Reference:** Unknown (2018) 'Inverse Reinforcement Learning Methods', arXiv:1806.06877.
**URL:** https://arxiv.org/pdf/1806.06877

## Policy Learning and Optimization

### Policy Gradient Methods
**Reference:** Sutton, R.S. and Barto, A.G. (2018) 'Reinforcement Learning: An Introduction', MIT Press, pp. 318-322.
**URL:** https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf
**Synopsis:** Introduction to policy gradient methods in reinforcement learning, discussing their theoretical foundations and practical applications.

### Q-Learning
**Reference:** Watkins, C.J. and Dayan, P. (1992) 'Q-Learning', Machine Learning, 8(3-4), pp. 279-292.
**URL:** https://www.sciencedirect.com/science/article/pii/B978008051080400005X
**Synopsis:** Overview of Q-learning, a popular off-policy temporal difference learning method used in reinforcement learning.

### Actor-Critic Methods
**Reference:** Konda, V.R. and Tsitsiklis, J.N. (2000) 'Actor-Critic Algorithms', SIAM Journal on Control and Optimization, 38(5), pp. 156-193.
**URL:** https://www.jstor.org/stable/2656768
**Synopsis:** Discussion of actor-critic methods, a class of policy gradient methods that combines value function estimation with policy evaluation.

### Policy Iteration
**Reference:** Sutton, R.S. and Barto, A.G. (2018) 'Reinforcement Learning: An Introduction', MIT Press, pp. 294-297.
**URL:** https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf
**Synopsis:** Introduction to policy iteration, a method for finding the optimal policy in reinforcement learning by iterating between policy evaluation and policy improvement.

### Policy Extraction
**Reference:** Bastani, O. et al. (2018) 'Verifiable Reinforcement Learning via Policy Extraction', CSAIL MIT.
**URL:** https://people.csail.mit.edu/asolar/papers/BastaniPS18.pdf

### Advanced Policy Extraction
**Reference:** Unknown (Unknown date) 'Advanced Policy Extraction Methods', OpenReview.
**URL:** https://openreview.net/pdf?id=Tk1VQDadfL

### DRL Policy Engineering
**Reference:** Unknown (2023) 'Deep RL Policy Engineering', arXiv:2305.16614.
**URL:** https://arxiv.org/pdf/2305.16614

### Policy Structure
**Reference:** Unknown (2023) 'Policy Structure in Reinforcement Learning', arXiv:2309.01884.
**URL:** https://arxiv.org/pdf/2309.01884

### Advanced Policy Methods
**Reference:** Unknown (2024) 'Advanced Policy Methods in RL', arXiv:2409.08687.
**URL:** https://arxiv.org/pdf/2409.08687

### Imitation Learning
**Reference:** Torabi, F. et al. (2019) 'Recent Advances in Imitation Learning from Observation', arXiv:1907.03976.
**URL:** https://arxiv.org/pdf/1907.03976

### Imitation Learning Applications
**Reference:** Unknown (2018) 'Imitation Learning Applications', ACM Digital Library.
**URL:** https://dl.acm.org/doi/pdf/10.1145/3197517.3201311

### Steering Vector Methods
**Reference:** Unknown (2024) 'Steering Vector Methods in Neural Networks', arXiv:2406.00045.
**URL:** https://arxiv.org/pdf/2406.00045

### Control Vector Methods
**Reference:** Unknown (2023) 'Control Vector Methods for LLMs', arXiv:2306.03341.
**URL:** https://arxiv.org/pdf/2306.03341

### Controlled Generation
**Reference:** Unknown (2023) 'Controlled Generation in Neural Networks', arXiv:2308.10248.
**URL:** https://arxiv.org/abs/2308.10248

### Activation Additions
**Reference:** Unknown (2022) 'Activation Additions for Neural Networks', arXiv:2205.05124.
**URL:** https://arxiv.org/pdf/2205.05124

### Steering Vector Applications
**Reference:** Unknown (2024) 'Applications of Steering Vectors', arXiv:2412.10427.
**URL:** https://arxiv.org/abs/2412.10427

### Controlled Outputs
**Reference:** Unknown (2023) 'Controlling Neural Network Outputs', arXiv:2311.06668.
**URL:** https://arxiv.org/abs/2311.06668

### Advanced Steering Methods
**Reference:** Unknown (2024) 'Advanced Neural Network Steering Methods', arXiv:2406.15518.
**URL:** https://arxiv.org/abs/2406.15518

## Meta-Learning and Hyperparameter Optimization

### Meta-Learning
**Reference:** Wang, J. and de Freitas, N. (2016) 'Learning to Learn with Gradient Descent by Gradient Descent', arXiv preprint arXiv:1606.04474.
**URL:** https://arxiv.org/pdf/1606.04474
**Synopsis:** Overview of meta-learning, the ability to learn to learn, and its application in reinforcement learning.

### Hyperparameter Optimization
**Reference:** Bergstra, J. and Bengio, Y. (2012) 'Random Search for Hyper-Parameter Optimization', Journal of Machine Learning Research, 13(1), pp. 281-305.
**URL:** https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf
**Synopsis:** Introduction to hyperparameter optimization, the process of selecting the best set of hyperparameters for a machine learning model.

### Meta-Gradients
**Reference:** Unknown (2025) 'Meta-Gradients in Reinforcement Learning', arXiv:2503.13751.
**URL:** https://arxiv.org/pdf/2503.13751

### Meta-Policy Gradients
**Reference:** Lange, R. (2020) 'Meta-Policy Gradients', Robert Lange's Blog.
**URL:** https://roberttlange.com/posts/2020/12/meta-policy-gradients/

### Advanced Meta-Gradients
**Reference:** Unknown (2024) 'Advanced Meta-Gradient Methods', arXiv:2406.19561.
**URL:** https://arxiv.org/pdf/2406.19561

### Meta-Learning Survey
**Reference:** Unknown (Unknown date) 'Meta-Learning Survey', IMDEA Networks Institute.
**URL:** https://dspace.networks.imdea.org/bitstream/handle/20.500.12761/1861/Thesis (3).pdf?sequence=1

### Hyperparameter Optimization
**Reference:** Unknown (2023) 'Hyperparameter Optimization Methods', arXiv:2306.01324.
**URL:** https://arxiv.org/pdf/2306.01324

### Augmented Modular RL
**Reference:** Unknown (Unknown date) 'Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge', ResearchGate.
**URL:** https://www.researchgate.net/publication/371290466_Augmented_Modular_Reinforcement_Learning_based_on_Heterogeneous_Knowledge

### Hyperparameter Tables
**Reference:** Unknown (Unknown date) 'Hyper-parameters of DQN on MiniGrid and MinAtar environments', ResearchGate.
**URL:** https://www.researchgate.net/figure/Hyper-parameters-of-DQN-on-MiniGrid-and-MinAtar-environments_tbl1_387671564

### Advanced Hyperparameter Methods
**Reference:** Unknown (2025) 'Advanced Hyperparameter Methods', arXiv:2503.10318v1.
**URL:** https://arxiv.org/pdf/2503.10318v1

## Grid World and Environment Design

### MiniGrid Environments
**Reference:** Chevalier-Boisvert, M., Willems, L. and Pal, S. (2018) 'Minimalistic Gridworld Environment for OpenAI Gym', GitHub repository.
**URL:** https://minigrid.farama.org/content/publications/

### GridWorld Task Complexity
**Reference:** Tanneberg, D. et al. (2023) 'Complexity of Reinforcement Learning Tasks', arXiv:2306.13831.
**URL:** https://arxiv.org/abs/2306.13831

### Grid-World Benchmark Environments
**Reference:** Chevalier-Boisvert, M. and Willems, L. (2018) 'Minimalistic Gridworld Environment for Gymnasium', arXiv:1711.09883.
**URL:** https://arxiv.org/pdf/1711.09883

## Neuroscience Connections

### Neural Correlates of RL
**Reference:** Richards, B.A. et al. (2022) 'A Deep Learning Framework for Neuroscience', Nature Reviews Neuroscience, 23, pp. 558-570.
**URL:** https://www.nature.com/articles/s41583-022-00642-0
**Synopsis:** Framework connecting deep learning and neuroscience, highlighting parallels between reinforcement learning algorithms and brain processes.

### Brain-Inspired Learning
**Reference:** Anonymous (2021) 'Brain-Inspired Learning Methods', Nature.
**URL:** https://www.nature.com/articles/s41586-021-04268-7

### Neural Population Dynamics
**Reference:** Saxena, S. and Cunningham, J.P. (2019) 'Towards the Neural Population Doctrine', Current Opinion in Neurobiology, 55, pp. 103-111.
**URL:** https://www.nature.com/articles/s41592-018-0109-9.pdf

### Brain-Inspired AI
**Reference:** Zador, A.M. (2023) 'Toward a Biological Solution to the Hard Problem of AI', Nature, 616, pp. 261-268.
**URL:** https://www.nature.com/articles/s41586-023-06031-6.pdf

### Neuroscience and Deep RL
**Reference:** Anonymous (2020) 'Neuroscience and Deep RL', NeurIPS.
**URL:** https://proceedings.neurips.cc/paper/2020/file/510f2318f324cf07fce24c3a4b89c771-Paper.pdf

### Recent Neuroscience Methods
**Reference:** Anonymous (2024) 'Recent Neuroscience Methods', Nature Methods.
**URL:** https://www.nature.com/articles/s41592-024-02582-2

## Advanced Transformer Research

### Transformer Architecture Details
**Reference:** Vaswani, A. et al. (2017) 'Attention Is All You Need', arXiv:1706.03762.
**URL:** https://arxiv.org/pdf/2106.09685

### Advanced Transformer Analysis
**Reference:** Anonymous (2022) 'Advanced Transformer Analysis', arXiv:2210.07229.
**URL:** https://arxiv.org/pdf/2210.07229

### Transformer Training Dynamics
**Reference:** Anonymous (2020) 'Transformer Training Dynamics', arXiv:2012.14913.
**URL:** https://arxiv.org/pdf/2012.14913

### Attention Mechanisms
**Reference:** Anonymous (2022) 'Attention Mechanisms in Transformers', arXiv:2202.05262.
**URL:** https://arxiv.org/pdf/2202.05262

### Transformer Efficiency
**Reference:** Anonymous (2024) 'Transformer Efficiency Methods', arXiv:2406.11263.
**URL:** https://arxiv.org/pdf/2406.11263

## Activation Engineering and Steering

### Activation Engineering
**Reference:** Anonymous (2024) 'Activation Engineering Methods', arXiv:2406.00045.
**URL:** https://arxiv.org/pdf/2406.00045 